In this report I go though my journey of impersonation attack prediction in the AWID dataset as presented in Moodle.
My aim is to provide justifications behind the decisions I take in order to reach a solution to this problem. 
Tables and figures contribute to the discussion and will be included in the appendix.
Additionally a process overview diagram is attached to the appendix in order to facilitate the reading of the report itself.

From a first look both the train and test sets suffer from having many zeros (well over 75% each). 
Given the large number of features captured this is to be expected as some parameters might not be available for each packet sampled. Nonetheless this exposes the risk of having 
poorly correlated features and weak dependency with the target variable. After all supervised estimators are built upon real world data and as such expect a good degree of
dependency between features. I'm using accuracy as a performance index to guide me towards the best classifier. This makes sense in this context as the class labels are balanced in both train and test.  

Figure1 and Figure2 represent the values in the dataset through an heatmap where values of zeros are coded in blue and values > 0 are assigned a red shifting gradient. 
Not just the figure displays the presence of many zeros, but also some features seem to have little to no variation of color throughout the dataset meaning that some of those might have constant values all the way through the samples. 

I decide to spin a few classifiers on the data so to have a feeling for their performance. 
I create a validation dataset by putting aside 1/3 of the training data , while keeping the rest of the unprocessed training data for training. 
I use a support vector classifier with RBF and default regularization parameter. 
This estimator gives me a 0.99 accuracy on both the training and validation sets while the accuracy on the real test is just over 52%.  

That not only indicates overfitting , but also shows that the estimate of the validation error is badly 
underestimating the true test error. 
This issue could potentially compromise model selection and model tuning since knowing what hurts and what helps is significally harder to achieve when everything performs well on both training and validation sets.
This leads me to consider pre-processing the data and perhaps apply some of the feature extraction and feature selection techniques learned in class. Those should take care of de-noising the data but also improve the overall performance of the algorithms.

I decide to pre-process the features set with normalization, so that all features have Euclidean length of 1. This ensures the features have the same scale, and also this 
should 'compress' possible outliers in the data. Effectively all sample are placed on the surface of a N-dimensional sphere of radius 1. 
This preprocessing technique seems to be the common denominator of several state of the arts papers on this issues , in particular [2] and [3].
This choice also makes sense since I intend to employ a K Nearest Neighbors Classifier , which can be very sensitive to outliers. 
KNN simply chooses the neighbors based on a distance criteria and is completely non-parametric,
meaning there are no assumption on the data. I hope that this choice will help me reducing bias of my model, and hopefully getting a more robust estimate 
of the test error with cross-validation.

 In order to prevent knowledge leakage from such learned normalization onto the test set I will use Pipeline.
Unfortunately even in this case with 10-fold cross-validation the estimated test error rate looks unrealistically high (nearly 100% accuracy).
while the accuracy on the test set is still unsatisfactory.
As it stands at the moment the high bias in cross-validation is preventing me from searching for optimal hyper-parameters via grid-search. 
The above discouraging results suggest that I need to seek for a better representation of the training feature set. 

One simple solution would be to drop training features which have zero variance. After removing such features from my training dataset I apply the same transformation
to the test set. Please note that I choose features to drop from training, by detecting constant features from the training dataset. I do not execute such assessment 
on the test set. Once I store the columns to be dropped in an array, I go ahead applying the transformation to the test set. 
Generally any transofrmation is "learned" on the training data and only applied on the test data as this is the unseen data , and knowledge should not be leaked from the test set to the training set. 
After applying the transformation training and test features drops down to 78 (from the original 152). 

As this point I will combine some unsupervised learning techniques for features extraction with feature selection and will bake them into a wrapper 
called FeatureUnion().  
I first apply normalization to the clean training data concatenate 10 features obtained by univariate selection (chi2) , 10 from PCA and 10 from Non-Negative Matrix Factorization and I chain the result to a KNN . 
There is still a huge problem with overfitting. 
 
To give myself another chance I use a similar technique by selecting a total of 40 features using  filter based feature selection and an additional 30 features dimensionality 
reduction (with Principal Component Analysis). I merge together to obtain a new training set with 78 features. 
Filter based (using chi2 univariate statistics) allows to check importance of features based on how strongly correlated with the target variable. It picks up intrinsic 
characteristics of the features and this is the reason why I'm choosing this over Recursive Feature Elimination (wrapper method) .Because RFE 'wraps' a separate estimator and fits it to the training data , my worry is that all features could be an excellent choise and would rank very high. I based this on experience from previous steps where I've witnessed severe overfitting with SVM  and KNN. 

I then choose normalizer to preprocess the data and I pipe the whole chain of tranformation into a linear SVM with stochastic gradient descend training. The reason I'm keen to use this different SVM classifier is that I expect stochastic gradient descend training to run faster while fitting the data on my modest 7th Gen Intel i5.
I also take advantage of a regularization parameter provided with the estimator interface (alpha parameter).  
The estimator performs quite well and seems to be very promising as I experience less overfitting and better accuracy by setting the strength of the regularization term 
10 times stronger than the default from scikit-learn.This reduces drastically overfitting 
with an estimated test accuracy of 98% with 10-fold cross validation and an accuracy on the test of 96.8%. This is also showing that cross-validation estimate is more reliable. 
 
I then check out a RandomForestClassifier on the same trasnformed data as for the previous SVM. I experience overfitting  
even using very large number of trees (800) each considering up to 5 randomly selected features for each bootstrap dataset .
Crossvalidation is out of question here because of the highly computational cost accociated with the algorithm. Because of this reason I do not explore further this model as it's not performing as well and it's very expensive.
 
I will try to build one last model, and see if I can improve test accuracy. 
I build 2 autoencoders with same topology but different activation functions. A plot of their structure is provided in the appendix. 
The first one is a vanilla autoencoder , it presents 2 hidden layers  for the encoder, 
a bottleneck and then grows back simmetrically for the decoder section. Rectified linear units activation functions are used throughout the autoencoder. 
The idea is to shrink the number of features down to a third of the original in order to allow it to build some form of internal representation.
The second one employes the same structure but with a different activation funciotn (LeakyRelu) , batch normalization layers which are supposed to help 
with gradient vanishing problem and also speed up training and activity regularizer in the bottleneck which should mute the output of some of the neurons in the layer in
a random fashion.
Despite the high tech involved in the second autoencoder it only does marginally better in terms of validation loss. 
Nonetheless I decide to stick with this one just to make the additional effort worth it and I find a compressed representation of the feature space with 26 features.
I merge the engineered features back to the other engineered data and I apply RFE to wrap the previous SVM. This time because I know that  the previous SVM on this particular
representation provides good unbiased estimate for the test error so it makes sense to use this wrapper as to select features. 
I select 78 best features, and I can see that all features generated from the autoencoder are selected. Please note my training data set has grown in terms of number of features.
In a way I try to disqualify those existent featires that can't compete anymore with newly engineered one, trying to simulate some sort of 
'natural selection' within the feature space.

Finally I get to building a neurual netwrk. The this process has invloved lots of trial and error. The main lesson learned is that having a powerful network with many layers 
loaded up with plenty of neurons it's definetely not the solution and also a bad idea in terms of computational resources. 
A plot of the topology of the network can be found in the appendix. Essentially 3 hidden layers with 5 , 4 and 3 neurons and 2 dropout layers prevent
overfitting yet giving room for modelling more complex non linear behavior. The network performance in terms of accuracy on the test set is the ball park of 96% 
while the cross validation estimate for test error is just below 98% . This model does not overfit , and in conjunction with the engineered data provides a robust platform. 
A key point in my opinion is that the model used is not as important. What matters is really the data engineering part of the problem. 
This model together with the previous SVM are good candidates and deserve some further analysis on their performance.











 (Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift) and leakyrelu activation function. It also take advantage of a regularizer 
 to add random de-activation of neurons in the bottleneck layer. 
 The other is more of a vanilla autoencoder. For their topology I invite to see fig X , fig Y in the appendix.

The BLABLA autoencoder performs better. I choose that to extract round(78/3) features. I then combine them with clean data, 30 selected features ,30 PCA-ED extracted features.
I use RFE with the SVM model I know performs better on a sample of that. The feautres  from the autoencoder latent space are indeed selected.
I keep 138 features, keeping the dimensionality the same I effectively swap in the new fearures from the autoencoder. 


*-- Neural Netwrok classifier
I create a NN with 3 drop out layers and activity regularizer on the output. Input data has only got the 10 features from the RFE. What I need to do is to get the combo dataset 
(138 features) concatenate the 10 from the autoencoder and then RFE again this time extracting 100 features.  

*-for later
Look for Random forest classifier
If performs better you can use it for RFE lather together with SVC
You then take original data (cleaned) add features with autoencoder ,and then use RFE with both SVM and CAST5  . Get it down to 5-6 features and use a NN classifier to classify.
If it all goes well you cna supply figures and descriptiive tables with accuracy (i.e. false positive rate , and recall ) . You could look into the O'reilly book (French man one)
Also listen to lesson about representation learning and coursework lesso again, and review your notes to add volume to the presentation. 
Taking out  zero variance variable to increase internal correlation of fwatuss.

Try to heatmap the clean data where y = 1 (attack) vs where y = 0 (normal traffic)



