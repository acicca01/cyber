Both the train and test set suffer from having many zeros (well over 75% ).
This can be due to some parameters not being available for all packets sampled. 
Figure 1 represents the value of the datasets through an heatmap where values of zeros are coded in white and values > 0 are assigned a red gradient. Not just the figure displays many zeros, but also some features seem to have little variation of color throughout the dataset. Also the features range between 0 and 1. 
I will split the training data so to check the accuracy of the held out validation set. And will also check how well the estimate of the error is when compare with the error rate on the test set. 
Given a support vector classifier with radial basis kernel , I get a 0.99 accuracy on the validation set. 
On the real test, however, the accuracy is a mere 0.50 . That not only indicates overfitting , but also shows that the estimate of the validation error is badly underestimating the true test error. 
This makes model selection and model tuning also very difficult.
Considering the number of zeros I decide to pre-process the features set with normalization , so that all features have length = 1. I will then use knn classifier and 10-fold cross validation in order to estimate the test error rate. 
In order to prevent knowledge leakage from the learned normalization onto the test set I will use Pipeline.
Unfortunately even in this case with 10-fold cross-validation the estimated test error rate looks unrealistically low (nearly 100% accuracy).
However normalizing the data and using KNeighbors Classifier has lifted the accuracy on the test set of about 3% .  
The above results suggest that I need to seek for a better representation of the training feature set. 
One simple solution would be to drop training features which are always zeros. I can then put the results of several extracted and selected feature together via FeatureUnion.
As it stands ,I would not use Recursive Feature Elimination to select attributes. This is because every model seems to be doing unrealistically well on the training data. 

*--model1
I use feature union to concatenate 30 features obtained by univariate selection (chi2) , PCA and Non-Negative Matrix Factorization and I chain the result to a KNN . 
There is still a huge problem with overfitting. 
 

*model2
I Choose 30 best features with chi  square , first 30 pricnipal component, and combine these together with the 78 non constant original features. 
I then choose normalizer to preprocess the data and I pipe it into a linear SVM with stochastic gradient descent training. 
The estimator performs quite well and seems to be very promising as I experience less overfitting and better accuracy.
I perform grid search to choose the optimal level of regularization and the penalty for misclassification. 
It turns out the cost of misclassifying a sample is very low (1.0000000000000003e-05 is preferred to the default 0.0001) , this is why I believe I'm able to reduce overfitting. 
 
I now have an accuracy of 98% on 10-fold cross validation while the accuracy on the test is 96.8%. This is also showing that cross-validation estimate is more reliable. 


*-for later
Look for Random forest classifier
If performs better you can use it for RFE lather together with SVC
You then take original data (cleaned) add features with autoencoder ,and then use RFE with both SVM and CAST5  . Get it down to 5-6 features and use a NN classifier to classify.
If it all goes well you cna supply figures and descriptiive tables with accuracy (i.e. false positive rate , and recall ) . You could look into the O'reilly book (French man one)
Also listen to lesson about representation learning and coursework lesso again, and review your notes to add volume to the presentation. 
