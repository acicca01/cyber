In this report I go though my journey of impersonation attack prediction in the AWID dataset as presented in Moodle.
My aim is to provide justifications behind the decisions I take in order to reach a solution to this problem. 
Tables and figures contribute to the discussion and will be included in the appendix.
Additionally a process flow diagram is attached to the appendix in order to facilitate the reading of the report itself.

From a first look both the train and test sets suffer from having many zeros (well over 75% each). 
Given the large number of features captured this is to be expected as some parameters might not be available for each packet sampled. Nonetheless this exposes the risk of having 
poorly correlated features and weak dependency with the target variable. After all supervised estimators are built upon real world data and as such expect a good degree of
relation (linear or non linear) between features.

Figure1 and Figure2 represent the values in the dataset through an heatmap where values of zeros are coded in white and values > 0 are assigned a red gradient. 
Not just the figure displays many zeros, but also some features seem to have little to no variation of color throughout the dataset meaning that some of those might
have constant values all the way through the samples. 

I create an validation dataset by putting aside 1/3 of the training data , while keeping the rest of the unprocessed training data for training. 
I use a support vector classifier to start my jounrey.

And will also check how well the estimate of the test error is when compared with the error rate on the test set. 
Given a support vector classifier with radial basis kernel , I get a 0.99 accuracy on the validation set. <----- LOOK AT THE C PARAMETER. MAYBE LOWER IT
. 
On the real test, however, the accuracy is a mere 0.50 . That not only indicates overfitting , but also shows that the estimate of the validation error is badly 
underestimating the true test error. 
This makes model selection and model tuning also very difficult. <RECHECK THIS-----> Using 10-fold cross validation and a logistic regression model gives me a 
slightly less biased estimate. Such estimate 
it's only marginally better and there is still a big problem with overfitting the training data. This leads me to consider pre-processing the data and perhaps apply some of the
feature extraction and feature selection techniques learned in class. Those should take care of de-noising the data but also improve the overall performance of the algorithms.

I decide to pre-process the features set with normalization, so that all features have Euclidean length of 1. This ensures the features have the same scale, and also this 
should 'compress' possible outliers in the data. Effectively all sample are placed on the surface of a N-dimensional sphere of radius 1. This choice makes sense since I intend 
to employ 
a K Nearest Neighbors Classifier , which can be very sensitive to outliers. KNN simply chooses the neighbors based on a distance criteria and is completely non-parametric,
meaning there are no assumption on the data. I hope that this choice will help me reducing bias of my model, and hopefully getting a more robust estimate 
of the test error with cross-validation.

 In order to prevent knowledge leakage from such learned normalization onto the test set I will use Pipeline.
Unfortunately even in this case with 10-fold cross-validation the estimated test error rate looks unrealistically high (nearly 100% accuracy).
However normalizing the data and using KNeighbors Classifier has lifted the accuracy on the real test set of about 3% compared to my previous attempts.
As it stands at the moment the high bias in cross-validation is preventing me from grid searching for optimal hyper-parameters. 
The above discouraging results suggest that I need to seek for a better representation of the training feature set. 

One simple solution would be to drop training features which have zero variance. After removing such features from my training dataset I apply the same transformation
to the test set. Please note that I choose features to drop from training, by detecting constant features from the training dataset. I do not execute such assessment 
on the test set. Once I store the columns to be dropped in an array, I go ahead applying the transformation to the test set. 
Generally any transofrmation is "learned" on the training data and only applied on the test data as this is the unseen data , and knowledge should not leak from the test set
to the training set. 
After applying the transformation training and test features drops down to 78 (from the original 152). I 

As this point I will combine some unsupervised learning techniques for features extraction with feature selection and will bake them into a wrapper 
called FeatureUnion().  
I first apply normalization to the clean training data concatenate 10 features obtained by univariate selection (chi2) , 10 from PCA and 10 from Non-Negative Matrix Factorization and I chain the result to a KNN . 
There is still a huge problem with overfitting. 
 
To give myself another chance I use a similar technique by extracitng a total of 60 features using both filter based feature selection and dimensinality 
reduction (with Principal Component Analysis).
Filter based (using chi2 univariate statistics) allows to check importance of features based on how strongly correlated with the target variable. It picks up intrinsic 
characteristics of the features and this is the reason why I'm choosing this over Recursive Feature Elimination (wrapper method) .Because RFE 'wraps' a separate estimator and fits it
to the training data , my worry is that all features will be an excellent choise and would rank very high. I based this on experience from previous steps where I've witnessed severe 
overfitting of SVM , logistic regression and KNN. 

Additionally I merge the newly extracted/selected features with the original non-constant features. This remerging technique it's also used by Aminanto et al. [1] 
together with normalization as pre-processing step. 

----------> ??? 
WHAT IS THE PONT OF SELECTING 30 FEATURES WITH FILTER AND THEN REMERGIN THEM INTO THE ORIGINAL . THAT MAKES THINGS NO GOOD FOR SURE. HOWEVER THIS COULD ALSO HINDER AT MY GOOD RESULT.
OR MAYBE DUPLICATION OF FEATURES IS KEY ??    <------------------

I then choose normalizer to preprocess the data and I pipe the whole chain of tranformation into a linear SVM with stochastic gradient descend training. 
I also take advantage of a regularization parameter provided with the estimator interface (alpha parameter).  
The estimator performs quite well and seems to be very promising as I experience less overfitting and better accuracy by setting the strength of the regularization term 
10 times stronger than the default from scikit-learn.This reduces drastically overfitting 
with an estimated test accuracy of 98% with 10-fold cross validation and an accuracy on the test of 96.8%. This is also showing that cross-validation estimate is more reliable. 
 
I then check out a RandomForestClassifier on the same trasnformed data as for the previous SVM. I can get 80% accuracy on test set and it seems to be overfitting <----HOW DO YOU KNOW?
even using very large number  of trees (800) each considering 
up to 50 randomly selected features for each bootstrap dataset .
Crossvalidation is out of question because of the computation cost accociated with the algorithm. Because of this reason I do not explore further this model as it's not performing as well
as he SGD SVm and also it's not efficient. 

I will try to build one last model, and see if I can improve test accuracy. I'm using accuracy as a guide to performance here as the class labels are balanced in both train
and test. 
I build 2 autoencoders with different (--->MAYBE SAME NEED TO MAKE A DECISION<----- )  topology and different activation functions. One is supposed to be more 
advanced and bsed on cutting edge batchnormalization 
 (Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift) and leakyrelu activation function. It also take advantage of a regularizer 
 to add random de-activation of  neurons in the bottleneck layer. 
 The other is more of a vanilla autoencoder. For their topology I invite to see fig X , fig Y in the appendix.

The BLABLA autoencoder performs better. I choose that to extract round(78/3) features. I then combine them with clean data, 30 selected features ,30 PCA-ED extracted features.
I use RFE with the SVM model I know performs better on a sample of that. The feautres  from the autoencoder latent space are indeed selected.
I keep 138 features, keeping the dimensionality the same I effectively swap in the new fearures from the autoencoder. 


*-- Neural Netwrok classifier
I create a NN with 3 drop out layers and activity regularizer on the output. Input data has only got the 10 features from the RFE. What I need to do is to get the combo dataset 
(138 features) concatenate the 10 from the autoencoder and then RFE again this time extracting 100 features.  

*-for later
Look for Random forest classifier
If performs better you can use it for RFE lather together with SVC
You then take original data (cleaned) add features with autoencoder ,and then use RFE with both SVM and CAST5  . Get it down to 5-6 features and use a NN classifier to classify.
If it all goes well you cna supply figures and descriptiive tables with accuracy (i.e. false positive rate , and recall ) . You could look into the O'reilly book (French man one)
Also listen to lesson about representation learning and coursework lesso again, and review your notes to add volume to the presentation. 
Taking out  zero variance variable to increase internal correlation of fwatuss.

Try to heatmap the clean data where y = 1 (attack) vs where y = 0 (normal traffic)



